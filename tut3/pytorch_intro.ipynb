{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbac375",
   "metadata": {},
   "source": [
    "# Introduction to Pytorch\n",
    "November, 2023\n",
    "\n",
    "In this tutorial, we will introduce the basic principles of Pytorch. \n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* 1) Tensors\n",
    "* 2) Autograd\n",
    "* 3) Models\n",
    "* 4) Training loop\n",
    "* 5) Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160f2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e076b8",
   "metadata": {},
   "source": [
    "### 1) Tensors\n",
    "Tensors are a specialized data structure that are similar to  NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators.\n",
    "Furthermore, tensors are optimized for automatic differentiation. \n",
    "\n",
    "Let's look at some basic manipulations of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([5., 3.])\n",
    "print(a)\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06e43a",
   "metadata": {},
   "source": [
    "Numpy's ndarrays and Pytorch's tensors are highly compatible and it is easy to switch between them. \n",
    "This is a way to create an object with the same undelying memory. \n",
    "This means that changes to the new tensor are reflected to the ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d45d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "print(f'Numpy array:\\n {a}')\n",
    "b = torch.from_numpy(a)\n",
    "print(f'Tensor created from the array:\\n {b}')\n",
    "b[:,0] = 8\n",
    "print(f'Numpy array after changes in the tensor:\\n {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11862d4",
   "metadata": {},
   "source": [
    "The conversion works also in the other direction, with the same rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea747db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(2,3)\n",
    "print(f'Tensor:\\n {b}')\n",
    "a = b.numpy()\n",
    "print(f'Numpy array:\\n {a}')\n",
    "a[:,0] = 1\n",
    "print(f'Numpy array:\\n {a}')\n",
    "print(f'Tensor array:\\n {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e6bff",
   "metadata": {},
   "source": [
    "If we do not wish the two objects to use the same undelying memory, torch.tensor() creates a copy of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = torch.tensor(a)\n",
    "print(f'Numpy array:\\n {a}')\n",
    "print(f'Tensor created from the array:\\n {b}')\n",
    "b[0,0]=8\n",
    "print(f'Numpy array after changes in the tensor:\\n {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a4bad",
   "metadata": {},
   "source": [
    "### 2) Autograd\n",
    "The backpropagation algorithm involves adjusting model parameters based on the gradient of the loss function relative to the specific parameter. \n",
    "To calculate these gradients, PyTorch utilizes an integrated differentiation engine called torch.autograd, which facilitates the automatic computation of gradients for any computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0daf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "l = torch.zeros(3)  # labels\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.mean(l - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd08744",
   "metadata": {},
   "source": [
    "w and b are the paramiters that we wish to optimize. \n",
    "To achieve this, it is essential to compute the gradients of the loss function wrt w and b.\n",
    "In order to do that, we set the requires_grad property of those tensors.\n",
    "This function, which is an Object, construct  a computational graph. \n",
    "This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for b = {b.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212daca3",
   "metadata": {},
   "source": [
    "To compute the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7131b803",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/serden/OneDrive/TUM/WS23/MLcomm/serdenTorch/tut3/pytorch_intro.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/serden/OneDrive/TUM/WS23/MLcomm/serdenTorch/tut3/pytorch_intro.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m w\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/serden/OneDrive/TUM/WS23/MLcomm/serdenTorch/tut3/pytorch_intro.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m b\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/serden/OneDrive/TUM/WS23/MLcomm/serdenTorch/tut3/pytorch_intro.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/serden/OneDrive/TUM/WS23/MLcomm/serdenTorch/tut3/pytorch_intro.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(w\u001b[39m.\u001b[39mgrad)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/serden/OneDrive/TUM/WS23/MLcomm/serdenTorch/tut3/pytorch_intro.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(b\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/OneDrive/TUM/WS23/MLcomm/serdenTorch/.venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/OneDrive/TUM/WS23/MLcomm/serdenTorch/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67407264",
   "metadata": {},
   "source": [
    "### 3) Models\n",
    "\n",
    "We create our **neural network** as a subclass of nn.Module, which is a base class for all neural network modules.\n",
    "This gives us more flexibility in network design.\n",
    "* Create NN as child class of nn.Module\n",
    "* Use super() to inherit all the methods and properties from the partent class\n",
    "* Define learnable parameters and forward propagation\n",
    "\n",
    "If not specified, all weights and biases will assume random initial values.\n",
    "\n",
    "There are other ways to create NNs, e.g. nn.sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d51fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10f9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(1, 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(2, 1)\n",
    "        self.lin1.weight = nn.Parameter(torch.Tensor([[-0.5], [1.5]])) # It is possible to assigned desired initial values\n",
    "        self.lin1.bias = nn.Parameter(torch.Tensor([0.5, -1]))\n",
    "        \n",
    "\n",
    "    def forward(self, y):\n",
    "        y = self.relu1(self.lin1(y))\n",
    "        return self.lin2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "871f4617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (lin1): Linear(in_features=1, out_features=2, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (lin2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the network\n",
    "net = SimpleNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ba4b9",
   "metadata": {},
   "source": [
    "To use the model, we pass directly the input data. This executes the model’s forward, along with some background operations. \n",
    "\n",
    "Do not call model.forward()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19e78158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the network tensor([-1.2888], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create input tensor\n",
    "y = torch.tensor(2.).reshape(1,)\n",
    "# Create target tensor()\n",
    "t = torch.tensor(3.5).reshape(1,)\n",
    "# Calculate output of the network\n",
    "xhat = net(y)\n",
    "print(f'Output of the network {xhat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef6f2b",
   "metadata": {},
   "source": [
    "### 4) Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c28bb6",
   "metadata": {},
   "source": [
    "We need an **optimizer** which is an object, that will hold the current state and will update the parameters based on the computed gradients.\n",
    "\n",
    "torch.optim support most common  optimization algorithms, such as (stochastic) gradient descent.\n",
    "\n",
    "Varoius gradient descent optimization algorithms exist and are used in practice, to overcome some of the gradient descent limitation, namely:\n",
    "* Choice of the learning rate\n",
    "* Adaptivity of the learning rate\n",
    "* Local minima\n",
    "\n",
    "Adaptive Moment Estimation (Adam) optimizer is usually a good starting choice,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22e48387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "optimizerAdam = optim.Adam(net.parameters(), lr =0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a010fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a loss function\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1956efa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to w1: tensor([-16.4207])\n",
      "w1 after the weght update: tensor([0.9304], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# repeat until convergence or for desired number of iterations (epochs):\n",
    "# forward propagation\n",
    "xhat = net(y)\n",
    "# compute loss\n",
    "loss = loss_fn(xhat,t)\n",
    "# reset gradient\n",
    "optimizerAdam.zero_grad()\n",
    "# backprop\n",
    "loss.backward()\n",
    "print(f'Gradient with respect to w1: {net.lin1.weight.grad[1]}')\n",
    "# update parameters\n",
    "optimizerAdam.step()\n",
    "print(f'w1 after the weght update: {net.lin1.weight[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4f1ce",
   "metadata": {},
   "source": [
    "### 5) Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2ba56",
   "metadata": {},
   "source": [
    "The DataLoader class in PyTorch that helps us to load and iterate over elements in a dataset.\n",
    "\n",
    "To improve the efficiency of the gradient descent algorithm, the data is genereally divided into subsets (**batches**), and the parameters of the networks gets updated calculating the gradient over the batch rather than the whole dataset (which could be huge). During the training, for every iteration over the entire dataset (**epoch**) we update the parameters for every batch.\n",
    "\n",
    "The DataLoader manages the data set for training such that we can extract the training batches from it during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34f731e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "x = 2 * np.random.rand(100_000) - 1\n",
    "y = np.tanh(x)\n",
    "# Convert data to tensors\n",
    "y_t = torch.Tensor(y).reshape(-1,1)\n",
    "x_t = torch.Tensor(x).reshape(-1,1)\n",
    "\n",
    "lr = 0.001  # learning rate\n",
    "batch_size = 1_000  \n",
    "num_epochs = 10\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(y_t, x_t),\n",
    "                                             batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77b4e503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Loss = 6.86e-03\n",
      "epoch 2: Loss = 4.98e-03\n",
      "epoch 4: Loss = 3.75e-03\n",
      "epoch 6: Loss = 2.96e-03\n",
      "epoch 8: Loss = 2.45e-03\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for j in range(num_epochs):\n",
    "    for (yi, xi) in dataloader:\n",
    "        x_hat = net(yi)\n",
    "        loss = loss_fn(x_hat, xi)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if j % 2 == 0:\n",
    "        x_hat = net(y_t)\n",
    "        loss = loss_fn(x_hat, x_t)\n",
    "        print(f'epoch {j}: Loss = {loss.detach().numpy() :.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c5521",
   "metadata": {},
   "source": [
    "It is in general a good practice to divide the available training data into 3 sets: training, validation and test.\n",
    "\n",
    "Trining data are used to train the network, valdation are used to tune the hyperparameters.\n",
    "Finally the test dataset can only be used once the training is over and we wish to evaluate the performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f68d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
